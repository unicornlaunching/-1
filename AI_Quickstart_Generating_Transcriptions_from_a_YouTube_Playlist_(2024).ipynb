{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMtS9JFIbvXFHwXwxe8iuJE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/unicornlaunching/-1/blob/master/AI_Quickstart_Generating_Transcriptions_from_a_YouTube_Playlist_(2024).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This code generates a zip file with the transcripts of all of the videos in a YouTube Playlist given a YouTube Playlist URL"
      ],
      "metadata": {
        "id": "5HM4y6Cs8wOF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Pytube"
      ],
      "metadata": {
        "id": "KI7pAQMu85t1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k85PvPVQHZeZ"
      },
      "outputs": [],
      "source": [
        "!pip install pytube"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import YouTube Playlist\n",
        "###### Remember to paste the URL of the YouTube Playlist where it says playlist_url"
      ],
      "metadata": {
        "id": "wxpbLT3o8uTA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pytube import Playlist\n",
        "import re\n",
        "\n",
        "def extract_video_ids(playlist_url):\n",
        "    playlist = Playlist(playlist_url)\n",
        "    video_ids = []\n",
        "    for video in playlist.video_urls:\n",
        "        video_id = re.search(r\"v=([a-zA-Z0-9_-]+)\", video).group(1)\n",
        "        video_ids.append(video_id)\n",
        "    return video_ids\n",
        "\n",
        "# Example usage\n",
        "playlist_url = \"https://www.youtube.com/playlist?list=PL2PXaLXFkx9_iTSs0HEjmwjr4DTgZAy7z\"\n",
        "video_ids = extract_video_ids(playlist_url)\n",
        "print(\"Video IDs:\", video_ids)"
      ],
      "metadata": {
        "id": "lRSaV_ScHg10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting Transcripts"
      ],
      "metadata": {
        "id": "K2oTkbOB6SBi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install youtube_transcript_api"
      ],
      "metadata": {
        "id": "8s7XKM8y6oV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from youtube_transcript_api import YouTubeTranscriptApi"
      ],
      "metadata": {
        "id": "4meuqPrT6KNY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# video_ids = ['26jyltSYayM', '2qT0T84092U', '36PWjcyXxaM', '8dTw8afd980', 'EuEpsVp5Lww', 'LXHcYNP7KjI', 'O1cjZjsF7bU', 'OisM0NiIJNA', 'THWgcVyI-R8', 'XFLgEUbigeI', 'YOR9QGgkkjs', 'ZI7lAazAje0', 'chVnS8qZWDo', 'dAaE8Fm_xYI', 'dTAMorqc7Ds', 'eTvzYNuZZeM', 'j4UHcjreQaM', 'lT9pIU9T3D4', 'lTeS_lcnZfc', 'ot8J-8AZ51k', 'rS5wdEIlx0U', 'yRqBZbbqy3Q', 'ynGpRHk29Qo', '3_LCiVsn8H0', '42T_IApk64g', '5ruEYRVqyn0', '6_C2tq6kjh8', '76cdUIl4cS8', '7UThXzy6Vyk', '80p29ip8X-w', '9ztIQHZHGe8', 'BLCPgWTY6PM', 'LCVE7uf8PV8', 'Q87iznkW5b4', 'RF_Lt0Adb9s', 'XCVoJvNWzo0', '_Tsc6EjxtDc', '_nbzrG_4Re0', 'dG-5EE_7Y0g', 'e8tTK6Ws4Kw', 'eNhhw59iYXQ', 'foADLhpZt3c', 'ggv13Whlchs', 'gv3gISRA5b0', 'gxHBTEPObzw', 'lRs_dsf2z60', 'vP5Lg9TRNUg', 'x3KmW0gAeHU', '2yhX3PsqKE4', '3tmOIdmfehc', '4s0EdODEq1g', 'FeSlLDPof8o', 'H5PvcHV-2a4', 'I-24DwHIRoc', 'Kfak815FOoE', 'NIleVngugQ0', 'P4foYEQUvWk', 'VgtMF5t7F0c', 'xUehVkNc4hc', 'DdCV7DyZVKU', 'G3a6dM5BI5I', 'ufSBDKNkZLY', 'FQmWp80Lt08', '5aj9HZRqctk', 'o3g8z3nuUcg']\n",
        "video_ids = ['26jyltSYayM', '2qT0T84092U', '36PWjcyXxaM', '8dTw8afd980', 'EuEpsVp5Lww', 'LXHcYNP7KjI', 'O1cjZjsF7bU', 'OisM0NiIJNA', 'THWgcVyI-R8', 'XFLgEUbigeI', 'YOR9QGgkkjs', 'ZI7lAazAje0', 'chVnS8qZWDo', 'dAaE8Fm_xYI', 'dTAMorqc7Ds', 'eTvzYNuZZeM', 'j4UHcjreQaM', 'lT9pIU9T3D4', 'lTeS_lcnZfc', 'ot8J-8AZ51k', 'rS5wdEIlx0U', 'yRqBZbbqy3Q', 'ynGpRHk29Qo', '3_LCiVsn8H0', '42T_IApk64g', '5ruEYRVqyn0', '6_C2tq6kjh8', '76cdUIl4cS8', '7UThXzy6Vyk', '80p29ip8X-w', '9ztIQHZHGe8', 'BLCPgWTY6PM', 'LCVE7uf8PV8', 'Q87iznkW5b4', 'RF_Lt0Adb9s', 'XCVoJvNWzo0', '_Tsc6EjxtDc', '_nbzrG_4Re0', 'dG-5EE_7Y0g', 'e8tTK6Ws4Kw', 'eNhhw59iYXQ', 'foADLhpZt3c', 'ggv13Whlchs', 'gv3gISRA5b0', 'gxHBTEPObzw', 'lRs_dsf2z60', 'vP5Lg9TRNUg', 'x3KmW0gAeHU', '2yhX3PsqKE4', '3tmOIdmfehc', '4s0EdODEq1g', 'FeSlLDPof8o', 'H5PvcHV-2a4', 'I-24DwHIRoc', 'Kfak815FOoE', 'NIleVngugQ0', 'P4foYEQUvWk', 'Ttr5CxwNSyk', 'VgtMF5t7F0c', 'WrKPKcmRaqA', 'cvYzWCpI7e4', 'd52EWrL2NMA', 'dSVJwqTqDNc', 'eYHLI4jdDv0', 'h9uoK__SU3Y', 'igdbI05Zhpc', 'l6-pJAshdZU', 'nV1rlIJgyAo', 'oBDCg0lTZlE', 'qXmQya5yzA0', 'ttFRl2Qzufk', 'uZFd9bo3xcg', 'wXR5hWvZf8A', 'xly_AXunKLo', '247CDvY6lQ0', '3KTxqNxT0ks', '5zvi7-gtJYY', '6YYfCtagYXY', '6p71Hqe1NZg', '7zG1j3n8ugs', '8l-rWqp06_U', 'AVrO-dNtDaQ', 'E6j-nMbEyH0', 'EQsMUzje24I', 'H7QBpOMY2Ho', 'Hf3p4vDmzMk', 'KNan-6_Xn9w', 'MeDPR-w-GKM', 'OBWEy1ptr2A', 'SbGy6h1JAiA', 'VMuTfjwtCjo', 'VO8k9imHBjg', 'VxIvJRNzS2A', 'bzDGeFlSGC0', 'e8ZvqLH7tB8', 'hce4MF73bW4', 'jvbe9UWS65E', 'nQfUuljbHGo', 'o0WxrJ6zkPo', 'uENXNCNCcGI', 'uPM8HzqQEHQ', 'xUehVkNc4hc', 'yF7RE7719pM', '0MeaoPnGfw8', '1VcelwipufA', '2cvPEjg1r3Y', '7M_G9xCr6ck', 'AKsxvYAyHQU', 'AMGPOF_rgA8', 'BPilIAmcAbE', 'DdCV7DyZVKU', 'Ei_eeyvciRk', 'G3a6dM5BI5I', 'K7o0XFADoeE', 'Kk0ruYbEdtw', 'Od3yj2KDg74', 'SmZSBks5qdI', 'Us8N0z3PMxk', '_EdZiSPKxQY', 'frlEJtBFrW8', 'iVgPL35pLXs', 'kemOEU_Vs84', 'pV38dSUSyOA', 'pZSbVKU4qjc', 'plClPYNoB7E', 'qAadAQgO3mE', 'qPaoJQab7ag', 'rGtBXoIBSH0', 'rvCt5G0Pbjw', 'ufSBDKNkZLY', 'xmuvxGMWGJY', 'ymPwoVdG8Kc', 'z-0ueGK_5OY', '0I0zH0_LFaE', '4XjeI6bcHRM', '7uBcI4OawUo', '9qRRt8-iZtg', 'DDPL5pObYeg', 'FQmWp80Lt08', 'GjKjYbntRGA', 'IVD0Jn7xey8', 'JYJCmL4kTjM', 'O7HOR6-ZXSo', 'SHYS-s1_I0o', 'T1B7p52oZR4', 'TjPnTxD3Lcw', 'aD4SxIRDj0Y', 'cASmNvpofw8', 'ebUhYYap1gk', 'i5u-vrDXHyU', 'jmy3FvjrXK0', 'mKBYqSQoaAw', 'mWk-p7ULwDw', 'nyWtMSnqGqE', 'oKmWX5Lbvx8', 'oqJerOLFypE', 'pUxIwr-N4nM', 'pY-zDYzHKZM', 'sw82EBrKmkI', 'vrOPm2c_zJA', 'wD7kfJrA1H8', 'wzWdNbkaR6I', '5aj9HZRqctk', 'o3g8z3nuUcg']"
      ],
      "metadata": {
        "id": "36ybTP9Q6WUP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_youtube_transcript(video_id):\n",
        "    try:\n",
        "        transcript_list = YouTubeTranscriptApi.get_transcript(video_id)\n",
        "        transcript = ' '.join([line['text'] for line in transcript_list])\n",
        "    except Exception as e:\n",
        "        print(f'Error retrieving transcript: {e}')\n",
        "        transcript = ''\n",
        "    return transcript"
      ],
      "metadata": {
        "id": "I-7aamcy7dTA"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving Each Transcript as its own text file in a zip."
      ],
      "metadata": {
        "id": "MlKDQauq9EXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from zipfile import ZipFile\n",
        "\n",
        "# Assuming get_youtube_transcript(video_id) function is defined somewhere\n",
        "\n",
        "# List of video IDs\n",
        "# video_ids = [...]  # Fill this with your video IDs\n",
        "\n",
        "# Directory to store text files\n",
        "output_dir = 'transcripts'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Iterate over each video ID\n",
        "for video_id in video_ids:\n",
        "    # Get transcript\n",
        "    transcript = get_youtube_transcript(video_id)\n",
        "    print('Transcript:', transcript)\n",
        "\n",
        "    # File name\n",
        "    file_name = os.path.join(output_dir, video_id + \".txt\")\n",
        "\n",
        "    # Write transcript to file\n",
        "    with open(file_name, \"w\") as file:\n",
        "        file.write(transcript)\n",
        "\n",
        "    print(f\"Text file '{file_name}' created.\")\n",
        "\n",
        "# Zip all text files\n",
        "with ZipFile('transcripts.zip', 'w') as zipf:\n",
        "    for root, _, files in os.walk(output_dir):\n",
        "        for file in files:\n",
        "            zipf.write(os.path.join(root, file), arcname=file)\n",
        "\n",
        "print(\"All text files zipped successfully.\")"
      ],
      "metadata": {
        "id": "L4UcJJxq7Tu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Single Text File of all Transcripts\n"
      ],
      "metadata": {
        "id": "p1A2240fJbu_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define the folder path\n",
        "folder_path = \"/content/transcripts\"\n",
        "\n",
        "# Define the output file path\n",
        "output_file_path = \"/content/combined_transcripts.txt\"\n",
        "\n",
        "# Open the output file in append mode\n",
        "with open(output_file_path, \"a\") as output_file:\n",
        "    # Iterate through each file in the folder\n",
        "    for file_name in os.listdir(folder_path):\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "        # Check if the file is a text file\n",
        "        if os.path.isfile(file_path) and file_name.endswith(\".txt\"):\n",
        "            with open(file_path, \"r\") as input_file:\n",
        "                # Read the content of the file and write it to the output file\n",
        "                output_file.write(input_file.read())\n",
        "                output_file.write(\"\\n\\n\")  # Add some separation between transcripts\n",
        "\n",
        "print(\"Combined transcripts saved to:\", output_file_path)\n"
      ],
      "metadata": {
        "id": "WniMBOGhJeH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Single PDF File of all Transcripts\n"
      ],
      "metadata": {
        "id": "SJ3b1hx7MZHL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install reportlab\n",
        "\n",
        "from reportlab.lib.pagesizes import letter\n",
        "from reportlab.pdfgen import canvas\n",
        "\n",
        "input_text_file = \"/content/combined_transcripts.txt\"\n",
        "output_pdf_file = \"/content/combined_transcripts.pdf\"\n",
        "\n",
        "# Create a PDF canvas\n",
        "c = canvas.Canvas(output_pdf_file, pagesize=letter)\n",
        "\n",
        "# Read the text file and write its content to the PDF\n",
        "with open(input_text_file, \"r\") as file:\n",
        "    lines = file.readlines()\n",
        "    for line in lines:\n",
        "        c.drawString(50, 750, line)\n",
        "        c.showPage()\n",
        "\n",
        "# Save the PDF\n",
        "c.save()\n",
        "\n",
        "print(\"PDF created successfully:\", output_pdf_file)\n"
      ],
      "metadata": {
        "id": "LmaiVxxnK3p0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}